{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58aeab40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a63452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load api key\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03bdeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 75042  100 75042    0     0   142k      0 --:--:-- --:--:-- --:--:--  143k\n",
      "curl: (6) Could not resolve host: data\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p 'data/paul_graham/'\n",
    "! curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd5726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-huggingface in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-llms-huggingface) (0.13.0)\n",
      "Requirement already satisfied: torch<3,>=2.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-llms-huggingface) (2.7.1)\n",
      "Requirement already satisfied: transformers<5,>=4.37.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (4.55.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.12.15)\n",
      "Requirement already satisfied: aiosqlite in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2025.7.0)\n",
      "Requirement already satisfied: httpx in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.2.0)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.3.2)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (11.3.0)\n",
      "Requirement already satisfied: platformdirs in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.0.42)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.14.1)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.20.1)\n",
      "Requirement already satisfied: griffe in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.9.0)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.4.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from torch<3,>=2.1.2->llama-index-llms-huggingface) (1.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.34.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (0.5.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5,>=4.37.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (1.1.7)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (1.9.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.10)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[torch]<5,>=4.37.0->llama-index-llms-huggingface) (5.9.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from sympy>=1.13.3->torch<3,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.26.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.4.6)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/venv/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-huggingface) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8f6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7febf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c3370d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797aec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from hf https://huggingface.co/Writer/camel-5b-hf\n",
    "from llama_index.core import PromptTemplate\n",
    "chat_prompt = PromptTemplate(\n",
    "    \"<|system|>\\nYou are a helpful assistant that answers questions based on the provided context.\\n<|user|>\\nContext: {context_str}\\n\\nQuestion: {query_str}\\n<|assistant|>\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "488fd551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31a6023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model\n",
      "loading file tokenizer.json from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/lakshitkhandelwal/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True, \"top_k\": 50, \"top_p\": 0.95},  # Updated parameters\n",
    "    query_wrapper_prompt=chat_prompt,  # Use chat prompt\n",
    "    tokenizer_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}  # Changed to bfloat16 like in docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bef077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.chunk_size = 512\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14026624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f6253a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author say in the essay?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "45e142b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author talks about how writing essays and publishing them online has changed the landscape for writers, offering a glimpse into the new generation of essays and the ways they can be published. The author also discusses the inspiration behind starting their own website, and their motivations for writing about topics outside their main field of expertise.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec0362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
